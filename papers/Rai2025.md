## Mechanistic Interpretability
Summary of the paper "A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models" by Daking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, and Ziyu Yao.

- **arXiv link**: [https://arxiv.org/abs/2407.02646](https://arxiv.org/abs/2407.02646)  
- **GitHub resource**: https://github.com/Dakingrai/awesome-mechanistic-interpretability-lm-papers  

---

### **Core Focus**
The paper provides a **comprehensive survey of Mechanistic Interpretability (MI)**, an emerging subfield aimed at understanding neural networks by **reverse-engineering their internal computations**. It specifically targets **transformer-based language models (LMs)**, which dominate modern NLP but remain opaque in their decision-making.

---

### **Motivation**
- Transformer LMs (e.g., GPT, LLaMA) have achieved state-of-the-art performance but raise concerns about **safety, alignment, and robustness** due to their black-box nature.
- MI offers a **bottom-up approach** to interpretability, contrasting with feature attribution or probing methods, by mapping model internals to **human-understandable mechanisms**.

---

### **Structure of the Survey**
The authors organise the review around a **task-centric taxonomy**, making it a roadmap for newcomers. The taxonomy covers:
1. **Fundamental Objects of Study**:
   - **Features**: Representations in neurons or attention heads.
   - **Circuits**: Compositional structures of features across layers.
   - **Universality**: Shared mechanisms across models.
2. **Techniques and Evaluation Methods**:
   - **Vocabulary Projection Methods**: Mapping activations to interpretable tokens.
   - **Intervention-Based Methods**: Causal testing via activation patching or ablation.
   - **Sparse Autoencoders (SAEs)**: Discovering interpretable latent features.
3. **Evaluation Metrics**:
   - Faithfulness, completeness, and robustness of discovered mechanisms.

---

### **Key Insights and Findings**
- **Circuits Hypothesis**: Transformer behaviour can often be decomposed into modular circuits (e.g., induction heads for in-context learning).
- **Sparse Feature Representations**: SAEs reveal interpretable features but face scalability challenges.
- **Intervention Studies**: Activation patching demonstrates causal roles of specific components, validating MI hypotheses.

---

### **Applications**
- **Safety and Alignment**: Detecting harmful behaviours or backdoors.
- **Model Editing**: Targeted interventions to modify behaviour.
- **Scientific Discovery**: Understanding emergent properties like in-context learning.

---

### **Challenges and Open Problems**
- **Scalability**: Current MI methods struggle with large-scale models.
- **Evaluation Standards**: Lack of unified benchmarks for MI quality.
- **Automation**: Heavy reliance on manual analysis limits progress.

---

### **Future Directions**
- **Automated Circuit Discovery**: Leveraging search and optimisation.
- **Cross-Model Generalisation**: Identifying universal circuits across architectures.
- **Integration with Training**: Embedding interpretability constraints during model development.