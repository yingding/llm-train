{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "# MPS fallback for torch CPU\n",
    "# https://github.com/pytorch/pytorch/issues/77764\n",
    "# https://stackoverflow.com/a/72416727\n",
    "# Notice: This is not a permanent solution, it's a workaround for the time being.\n",
    "# It must be run as the first line of the script.\n",
    "# DEVICE = \"mps\"\n",
    "DEVICE = \"npu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"c:\\Users\\yingdingwang\\Documents\\VENV\\gpt3.12\\Lib\\site-packages\\torch\\lib\\aoti_custom_ops.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mapplyllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     AcceleratorHelper,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# DIR_MODE_MAP\u001b[39;00m\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mapplyllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     time_func\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yingdingwang\\Documents\\VENV\\gpt3.12\\Lib\\site-packages\\applyllm\\__init__.py:232\u001b[0m\n\u001b[0;32m    215\u001b[0m PY3 \u001b[38;5;241m=\u001b[39m (sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# from .accelerator.accelerator_utils import (\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m#     DirectorySetting, DIR_MODE_MAP, TokenHelper, AcceleratorHelper,\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m#     AcceleratorStatus, MpsAcceleratorStatus, CudaAcceleratorStatus,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# from .io import *\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# from .pipelines import *\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accelerators, io, pipelines, utils\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# https://stackoverflow.com/questions/44834/what-does-all-mean-in-python/35710527#35710527\u001b[39;00m\n\u001b[0;32m    235\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.0.7\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\yingdingwang\\Documents\\VENV\\gpt3.12\\Lib\\site-packages\\applyllm\\accelerators\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerator_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     DirectorySetting, DIR_MODE_MAP, TokenHelper, AcceleratorHelper,\n\u001b[0;32m      3\u001b[0m     AcceleratorStatus, MpsAcceleratorStatus, CudaAcceleratorStatus,\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [ DirectorySetting, DIR_MODE_MAP, TokenHelper, AcceleratorHelper,\n\u001b[0;32m      7\u001b[0m             AcceleratorStatus, MpsAcceleratorStatus, CudaAcceleratorStatus,]\n",
      "File \u001b[1;32mc:\\Users\\yingdingwang\\Documents\\VENV\\gpt3.12\\Lib\\site-packages\\applyllm\\accelerators\\accelerator_utils.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msubprocess\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yingdingwang\\Documents\\VENV\\gpt3.12\\Lib\\site-packages\\torch\\__init__.py:262\u001b[0m\n\u001b[0;32m    258\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    260\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 262\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\yingdingwang\\Documents\\VENV\\gpt3.12\\Lib\\site-packages\\torch\\__init__.py:258\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    254\u001b[0m             err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    255\u001b[0m             err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    256\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    257\u001b[0m             )\n\u001b[1;32m--> 258\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    260\u001b[0m kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"c:\\Users\\yingdingwang\\Documents\\VENV\\gpt3.12\\Lib\\site-packages\\torch\\lib\\aoti_custom_ops.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "from applyllm.accelerators import (\n",
    "    AcceleratorHelper,\n",
    "    # DIR_MODE_MAP\n",
    ")\n",
    "from applyllm.utils import (\n",
    "    time_func\n",
    ")\n",
    "import os, sys\n",
    "\n",
    "# path for windows\n",
    "from win_patch import (\n",
    "    DirectorySetting,\n",
    "    DIR_MODE_MAP\n",
    ")\n",
    "# TODO: rename the init_mps_torch to init_env_torch(dir_setting: DirectorySetting)\n",
    "AcceleratorHelper.init_mps_torch(dir_setting=DIR_MODE_MAP[\"win_local\"])\n",
    "\n",
    "print(os.environ['XDG_CACHE_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_npu_acceleration_library import NPUModelForCausalLM, int8\n",
    "from intel_npu_acceleration_library.compiler import CompilerConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print gpt2 model architecture\n",
    "w weight for token embedding [50257, 768] - look up table for tokens:\n",
    "* 50257 tokens in gpt2 vocabulary, \n",
    "* 768 dimension embedding for each token - distriburted representation stands in for that token\n",
    "\n",
    "w weight for positions embedding [1024, 768] - look up table for positions:\n",
    "* gpt-2 has max token length of 1024 token - 1024 positions each token can be attending to in the past\n",
    "* 768 parameters as position embedding for each of this position is learned by optimization\n",
    "\n",
    "```\n",
    "transformer.h.0.ln_1.weight torch.Size([768])\n",
    "transformer.h.0.ln_1.bias torch.Size([768])\n",
    "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.0.ln_2.weight torch.Size([768])\n",
    "transformer.h.0.ln_2.bias torch.Size([768])\n",
    "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
    "```\n",
    "are weights and biases of the transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler_conf = CompilerConfig(dtype=int8)\n",
    "compiler_conf = CompilerConfig(dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M parameters\n",
    "model_hf = NPUModelForCausalLM.from_pretrained(\"gpt2\", config=compiler_conf) # 124M parameters\n",
    "sd_hf = model_hf.state_dict() # raw tensors \n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "    # v is the tensor (values)\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### view weight tensor\n",
    "flatten the tensor weights in Pytorch, flatten it, and slice the last 20 float values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sd_hf[\"transformer.wpe.weight\"].view(-1) flatten the tensor from Pytorch to 1D\n",
    "sd_hf[\"transformer.wpe.weight\"].view(-1)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the weights values of position embeddings\n",
    "There are structures in the position embeddings of gpt-2 tokens\n",
    "* Y axis is the position, each row indicate a position of 1024 in gpt-2 input\n",
    "* X axis is the representation of the position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(sd_hf[\"transformer.wpe.weight\"], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot columns of the position embedding weights\n",
    "We look at the channel as a function of position from [0, 1023]\n",
    "\n",
    "The channels are more or less response to the position\n",
    "* green channel like to fire for every position after 200 till 800\n",
    "* since the channel position function are noicy/jagged, you can tell this model is not fully trained\n",
    "* The more trained this model was, the more you would expect channel position function to smooth out\n",
    "* At the beginning of the optimization, this curves are complete random noise, because the position embedding table is initialized completely at random\n",
    "* The channel curve of position embedding looks sinusoidal like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 150])\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 200])\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the attention matrix of the first layer with a block of 300x300\n",
    "plt.imshow(sd_hf[\"transformer.h.1.attn.c_attn.weight\"][:300,:300], cmap=\"gray\")\n",
    "# there is some structure in the attention matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set seed for transformer to get the generation fixed\n",
    "\n",
    "With `set_seed(42)`\n",
    "you shall see the model allways generate the same output:\n",
    "\n",
    "```console\n",
    "[{'generated_text': \"Hello, I'm a language model, I'm writing a new language for you. But first, I'd like to tell you about the language itself\"},\n",
    " {'generated_text': \"Hello, I'm a language model, and I'm trying to be as expressive as possible. In order to be expressive, it is necessary to know\"},\n",
    " {'generated_text': \"Hello, I'm a language model, so I don't get much of a license anymore, but I'm probably more familiar with other languages on that\"},\n",
    " {'generated_text': \"Hello, I'm a language model, a functional model... It's not me, it's me!\\n\\nI won't bore you with how\"},\n",
    " {'generated_text': \"Hello, I'm a language model, not an object model.\\n\\nIn a nutshell, I need to give language model a set of properties that\"}]\n",
    " ```\n",
    "\n",
    " Notice:\n",
    " * if I change the device from CPU to MPS, the results will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a pipeline to use the model to sample the text\n",
    "from transformers import pipeline, set_seed \n",
    "# generator = pipeline('text-generation', model=\"gpt2\", device_map=DEVICE) # use the default model\n",
    "\n",
    "'''\n",
    "set the seed for reproducibility, if seed is set. the output will be the same\n",
    "if the code of weights is the same, with fixed seed, the output will be the same\n",
    "'''\n",
    "# set_seed(42)\n",
    "\n",
    "'''\n",
    "# \"Hello...\" is prefix, sampling 30 tokens, and return 5 output sequences\n",
    "# 5 different completions of the prefix\n",
    "# Truncation was not explicitly activated but `max_length` is provided a specific value, \n",
    "# please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. \n",
    "# If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely\n",
    "# by providing a specific strategy to `truncation`.\n",
    "# Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
    "'''\n",
    "# generator(\"Hello, I'm a language model,\", max_length=30, truncation=True, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_func\n",
    "def chat(prompt=\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5, device_map=\"cpu\"):\n",
    "    generator = pipeline('text-generation', model=\"gpt2\", device_map=device_map)\n",
    "    set_seed(42)\n",
    "    return generator(prompt, max_length=max_length, truncation=True, num_return_sequences=num_return_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5, device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"what's your name?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat(prompt=prompt, max_length=30, num_return_sequences=5, device_map=\"npu\")\n",
    "chat(prompt=prompt, max_length=30, num_return_sequences=5, device_map=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
