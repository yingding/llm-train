# Learning source

* Lets reproduce GPT-2 https://www.youtube.com/watch?v=l8pRSuU81PU

## GPT-2

GPT-2 is a decoder only transformer

GPT-2 has no encoder

## Learning sections
* GPT-2 model code generation test (41:23) https://youtu.be/l8pRSuU81PU?t=2483
* GPT-2 architecture workthrough (13:56) https://youtu.be/l8pRSuU81PU?t=836
* Attension implementation (24:00) https://youtu.be/l8pRSuU81PU?t=1440

## online openai tiktokenizer
* https://tiktokenizer.vercel.app

## monitor on apple silicon acceletor
```shell
sudo asitop
```
* asitop https://github.com/tlkh/asitop

## References:
* GPT-2 paper https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
* GPT-3 paper https://arxiv.org/abs/2005.14165
* Attention Is All You Need (original transformer paper) https://arxiv.org/abs/1706.03762
* PyTorch GELU https://pytorch.org/docs/stable/generated/torch.nn.GELU.html
* GELU paper https://arxiv.org/abs/1606.08415





