{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "# MPS fallback for torch CPU\n",
    "# https://github.com/pytorch/pytorch/issues/77764\n",
    "# https://stackoverflow.com/a/72416727\n",
    "# Notice: This is not a permanent solution, it's a workaround for the time being.\n",
    "# It must be run as the first line of the script.\n",
    "# DEVICE = \"mps\"\n",
    "# DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yingding/MODELS\n"
     ]
    }
   ],
   "source": [
    "from applyllm.accelerators import (\n",
    "    AcceleratorHelper,\n",
    "    DIR_MODE_MAP\n",
    ")\n",
    "from applyllm.utils import (\n",
    "    time_func\n",
    ")\n",
    "import os\n",
    "\n",
    "# set up the torch mps environment and huggingface cache home, before importing datasets and transformers\n",
    "AcceleratorHelper.init_mps_torch(dir_setting=DIR_MODE_MAP.get(\"mac_local\"))\n",
    "\n",
    "print(os.environ['XDG_CACHE_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate response:\n",
    "* https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/examples/generate_response.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24fb60c0919d4c069e8ec9eaf714e246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Hello, I'm a language model,\n",
      " and.\n",
      "[ [ * ] 1.1 0.0 0.01.02 0 ::.:.05 0 3.5 2 0\n",
      "==========\n",
      "Prompt: 123.053 tokens-per-sec\n",
      "Generation: 116.666 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "# model, tokenizer = load(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "model_hf, tokenizer = load(\"gpt2\") # 124M parameters\n",
    "\n",
    "generation_args = {\n",
    "    \"temp\": 0.6,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"repetition_context_size\": 20,\n",
    "    \"top_p\": 0.95,\n",
    "    # \"num_return_sequences\": 5,\n",
    "}\n",
    "\n",
    "response = generate(\n",
    "    model=model_hf, \n",
    "    tokenizer=tokenizer, \n",
    "    prompt=\"Hello, I'm a language model,\", \n",
    "    max_tokens=30,                \n",
    "    verbose=True,\n",
    "    **generation_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print gpt2 model architecture\n",
    "w weight for token embedding [50257, 768] - look up table for tokens:\n",
    "* 50257 tokens in gpt2 vocabulary, \n",
    "* 768 dimension embedding for each token - distriburted representation stands in for that token\n",
    "\n",
    "w weight for positions embedding [1024, 768] - look up table for positions:\n",
    "* gpt-2 has max token length of 1024 token - 1024 positions each token can be attending to in the past\n",
    "* 768 parameters as position embedding for each of this position is learned by optimization\n",
    "\n",
    "```\n",
    "transformer.h.0.ln_1.weight torch.Size([768])\n",
    "transformer.h.0.ln_1.bias torch.Size([768])\n",
    "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.0.ln_2.weight torch.Size([768])\n",
    "transformer.h.0.ln_2.bias torch.Size([768])\n",
    "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
    "```\n",
    "are weights and biases of the transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sd_hf \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_hf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m() \u001b[38;5;66;03m# raw tensors \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m sd_hf\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# v is the tensor (values)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(k, v\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/VENV/gpt3.12/lib/python3.12/site-packages/mlx/nn/layers/base.py:137\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mModule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "# model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M parameters\n",
    "sd_hf = model_hf.state_dict() # raw tensors \n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "    # v is the tensor (values)\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### view weight tensor\n",
    "flatten the tensor weights in Pytorch, flatten it, and slice the last 20 float values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sd_hf[\"transformer.wpe.weight\"].view(-1) flatten the tensor from Pytorch to 1D\n",
    "sd_hf[\"transformer.wpe.weight\"].view(-1)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the weights values of position embeddings\n",
    "There are structures in the position embeddings of gpt-2 tokens\n",
    "* Y axis is the position, each row indicate a position of 1024 in gpt-2 input\n",
    "* X axis is the representation of the position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(sd_hf[\"transformer.wpe.weight\"], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot columns of the position embedding weights\n",
    "We look at the channel as a function of position from [0, 1023]\n",
    "\n",
    "The channels are more or less response to the position\n",
    "* green channel like to fire for every position after 200 till 800\n",
    "* since the channel position function are noicy/jagged, you can tell this model is not fully trained\n",
    "* The more trained this model was, the more you would expect channel position function to smooth out\n",
    "* At the beginning of the optimization, this curves are complete random noise, because the position embedding table is initialized completely at random\n",
    "* The channel curve of position embedding looks sinusoidal like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 150])\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 200])\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the attention matrix of the first layer with a block of 300x300\n",
    "plt.imshow(sd_hf[\"transformer.h.1.attn.c_attn.weight\"][:300,:300], cmap=\"gray\")\n",
    "# there is some structure in the attention matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set seed for transformer to get the generation fixed\n",
    "\n",
    "With `set_seed(42)`\n",
    "you shall see the model allways generate the same output:\n",
    "\n",
    "```console\n",
    "[{'generated_text': \"Hello, I'm a language model, I'm writing a new language for you. But first, I'd like to tell you about the language itself\"},\n",
    " {'generated_text': \"Hello, I'm a language model, and I'm trying to be as expressive as possible. In order to be expressive, it is necessary to know\"},\n",
    " {'generated_text': \"Hello, I'm a language model, so I don't get much of a license anymore, but I'm probably more familiar with other languages on that\"},\n",
    " {'generated_text': \"Hello, I'm a language model, a functional model... It's not me, it's me!\\n\\nI won't bore you with how\"},\n",
    " {'generated_text': \"Hello, I'm a language model, not an object model.\\n\\nIn a nutshell, I need to give language model a set of properties that\"}]\n",
    " ```\n",
    "\n",
    " Notice:\n",
    " * if I change the device from CPU to MPS, the results will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a pipeline to use the model to sample the text\n",
    "from transformers import pipeline, set_seed \n",
    "# generator = pipeline('text-generation', model=\"gpt2\", device_map=DEVICE) # use the default model\n",
    "\n",
    "'''\n",
    "set the seed for reproducibility, if seed is set. the output will be the same\n",
    "if the code of weights is the same, with fixed seed, the output will be the same\n",
    "'''\n",
    "# set_seed(42)\n",
    "\n",
    "'''\n",
    "# \"Hello...\" is prefix, sampling 30 tokens, and return 5 output sequences\n",
    "# 5 different completions of the prefix\n",
    "# Truncation was not explicitly activated but `max_length` is provided a specific value, \n",
    "# please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. \n",
    "# If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely\n",
    "# by providing a specific strategy to `truncation`.\n",
    "# Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
    "'''\n",
    "# generator(\"Hello, I'm a language model,\", max_length=30, truncation=True, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_func\n",
    "def chat(prompt=\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5, device_map=\"cpu\"):\n",
    "    generator = pipeline('text-generation', model=\"gpt2\", device_map=device_map)\n",
    "    set_seed(42)\n",
    "    return generator(prompt, max_length=max_length, truncation=True, num_return_sequences=num_return_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5, device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5, device_map=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
